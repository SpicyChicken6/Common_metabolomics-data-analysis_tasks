{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pkgutil\n",
    "import csv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# these functions should be used to determine if log transformations are needed for the data\n",
    "\n",
    "\n",
    "def normal_dist_check(input_data):\n",
    "    \"\"\"\n",
    "    Checks for normality in the data by performing a shapiro test.\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        true if the data is normal based on shapiro test, false otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    data_to_check = input_data.iloc[:, 2:]\n",
    "    result = stats.shapiro(data_to_check)\n",
    "\n",
    "    return result[1] < 0.05\n",
    "\n",
    "\n",
    "# read the test dataset from the package into a dataframe\n",
    "def read_data_file(\n",
    "    package_name=\"metabolomics_analysis_tools\",\n",
    "    file_path=\"resources/test_dataset/human_cachexia.csv\",\n",
    "):\n",
    "    if not file_path:\n",
    "        return \"Error: empty input\"\n",
    "    data_bytes = pkgutil.get_data(package_name, file_path)\n",
    "    if data_bytes is None:\n",
    "        raise FileNotFoundError(\"Could not find data file\")\n",
    "\n",
    "    else:\n",
    "        print(\"data read successfully\")\n",
    "        data_str = data_bytes.decode(\"utf-8\")\n",
    "        data_file = StringIO(data_str)\n",
    "        csv_reader = csv.reader(data_file)\n",
    "        rows = [row for row in csv_reader]\n",
    "        df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "        df.iloc[:, 2:] = df.iloc[:, 2:].astype(\n",
    "            {col: float for col in df.iloc[:, 2:].columns}\n",
    "        )\n",
    "        print(\"the shape of the dataframe is: \", df.shape)\n",
    "        return df\n",
    "    \n",
    "\n",
    "# normalization methods\n",
    "def normalize_by_sum(input_data):\n",
    "    \"\"\"\n",
    "    Normalize the data by dividing each column by the sum of the row\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Normalized dataframe\n",
    "    \"\"\"\n",
    "    data_to_process = input_data.iloc[:, 2:]\n",
    "    normalized_data = data_to_process.div(data_to_process.sum(axis=0), axis=1)\n",
    "    normed_data = input_data.copy()\n",
    "    normed_data.iloc[:, 2:] = normalized_data\n",
    "\n",
    "    return normed_data\n",
    "\n",
    "\n",
    "def normalize_by_median(input_data):\n",
    "    \"\"\"\n",
    "    Normalize the data by dividing each column by the median of the row\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Normalized dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    data_to_process = input_data.iloc[:, 2:]\n",
    "    normalized_data = data_to_process.div(data_to_process.median(axis=0), axis=1)\n",
    "    normed_data = input_data.copy()\n",
    "    normed_data.iloc[:, 2:] = normalized_data\n",
    "\n",
    "    return normed_data\n",
    "\n",
    "\n",
    "def normalize_by_reference_sample_PQN(input_data):\n",
    "    \"\"\"\n",
    "    Normalize the data by dividing each column by the PQN of the reference sample.\n",
    "    PQN has been shown to be effective at normalizing data from different platforms and technologies,\n",
    "    and can reduce batch effects and other systematic variations in data. However,\n",
    "    PQN assumes that most genes or proteins are not differentially expressed, which may not be true in all cases.\n",
    "    Additionally, PQN may not be appropriate for all types of data,\n",
    "    and other normalization methods may be more appropriate\n",
    "    depending on the specific research question and experimental design.\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Normalized dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    data_to_process = input_data.iloc[:, 2:]\n",
    "    sample_medians = data_to_process.median(axis=0)\n",
    "    data_norm = data_to_process.div(sample_medians, axis=1)\n",
    "    metabolite_means = np.exp(np.log(data_norm).mean(axis=1))\n",
    "    data_norm = data_norm.div(metabolite_means, axis=0)\n",
    "    col_medians = data_norm.median(axis=0)\n",
    "    data_norm = data_norm.div(col_medians, axis=1)\n",
    "    normed_data = input_data.copy()\n",
    "    normed_data.iloc[:, 2:] = data_norm\n",
    "\n",
    "    return normed_data\n",
    "\n",
    "def data_scaling_mean_centered(normalized_data):\n",
    "    \"\"\"\n",
    "    Scale the data by subtracting the mean of each column from each value in the column\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Scaled dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    data_scaled = normalized_data.iloc[:, 2:].subtract(\n",
    "        normalized_data.mean(axis=0), axis=1\n",
    "    )\n",
    "    scaled_data = normalized_data.copy()\n",
    "    scaled_data.iloc[:, 2:] = data_scaled\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "def data_transformation_log(input_data):\n",
    "    \"\"\"\n",
    "    Transform the data by taking the log10 of each value\n",
    "\n",
    "    Args:\n",
    "        input_data (pandas.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    data_to_process = input_data.iloc[:, 2:]\n",
    "\n",
    "    if (data_to_process <= 0).any().any():\n",
    "        raise ValueError(\"Error: Non-positive values found in input data\")\n",
    "    data_log10 = data_to_process.apply(np.log10)\n",
    "    transformed_data = input_data.copy()\n",
    "    transformed_data.iloc[:, 2:] = data_log10\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "def PCA_analysis(normalized_data, number_of_components=2):\n",
    "    data_to_process = normalized_data.iloc[:, 2:]\n",
    "    pca = PCA(n_components=number_of_components)\n",
    "    principal_components = pca.fit_transform(data_to_process)\n",
    "    principal_df = pd.DataFrame(\n",
    "        data=principal_components, columns=[f\"PC {i+1}\" for i in range(number_of_components)]\n",
    "    )\n",
    "    groups = normalized_data.iloc[:, 1]\n",
    "\n",
    "    # Create a LabelEncoder to encode the groups as integers\n",
    "    le = LabelEncoder()\n",
    "    groups_encoded = le.fit_transform(groups)\n",
    "\n",
    "    # Create a colormap based on the number of unique groups\n",
    "    cmap = plt.cm.get_cmap('viridis', len(np.unique(groups_encoded)))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(\n",
    "        principal_df[\"PC 1\"], principal_df[\"PC 2\"], c=groups_encoded, cmap=cmap\n",
    "    )\n",
    "\n",
    "    # Create a legend with the group labels\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color=cmap(i), label=label, markersize=7, linestyle='')\n",
    "                       for i, label in enumerate(le.classes_)]\n",
    "    legend1 = ax.legend(handles=legend_elements, title=\"Groups\")\n",
    "    ax.add_artist(legend1)\n",
    "    ax.set_xlabel(\"PC 1\")\n",
    "    ax.set_ylabel(\"PC 2\")\n",
    "    ax.set_title(\"PCA of Metabolomic Data\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def volcano_plot(\n",
    "    grouped_data,\n",
    "    sig_threshold=0.05,\n",
    "    fold_change_threshold=1.5,\n",
    "    group_name=\"Muscle loss\",\n",
    "    group_A_name=\"control\",\n",
    "    group_B_name=\"cachexic\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a two-sample t-test on each feature and plot the results as a volcano plot\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the data to float\n",
    "    grouped_data = grouped_data.iloc[:, 1:]\n",
    "    grouped_data.iloc[:, 1:] = grouped_data.iloc[:, 1:].astype(float)\n",
    "    # set significance threshold and fold change threshold\n",
    "    sig_threshold = 0.05\n",
    "    fold_change_threshold = 1.5\n",
    "\n",
    "    # create a new DataFrame to store the results\n",
    "    result_grouped_data = pd.DataFrame(\n",
    "        columns=[\"Metabolite\", \"Fold Change\", \"P-value\", \"FDR\"]\n",
    "    )\n",
    "\n",
    "    # iterate over each feature\n",
    "    for col in grouped_data.iloc[:, 1:]:\n",
    "        group_A = grouped_data[grouped_data[group_name] == group_A_name][col]\n",
    "        group_B = grouped_data[grouped_data[group_name] == group_B_name][col]\n",
    "\n",
    "        # perform a two-sample t-test for each feature\n",
    "        t_stat, p_val = stats.ttest_ind(group_A, group_B)\n",
    "\n",
    "        # calculate the fold change\n",
    "        mean_A = group_A.mean()\n",
    "        mean_B = group_B.mean()\n",
    "        fold_change = mean_B / mean_A\n",
    "\n",
    "        # correct the p-value for multiple testing using Benjamini-Hochberg procedure\n",
    "        p_vals = [p_val]\n",
    "        corrected_p_vals = multipletests(p_vals, method=\"fdr_bh\")[1][0]\n",
    "\n",
    "        # add the results to the result DataFrame\n",
    "        result_grouped_data = result_grouped_data.append(\n",
    "            {\n",
    "                \"Metabolite\": col,\n",
    "                \"Fold Change\": fold_change,\n",
    "                \"P-value\": p_val,\n",
    "                \"FDR\": corrected_p_vals,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # filter significant features based on FDR and fold change threshold\n",
    "    sig_features = result_grouped_data[\n",
    "        (result_grouped_data[\"FDR\"] < sig_threshold)\n",
    "        & (abs(result_grouped_data[\"Fold Change\"]) > fold_change_threshold)\n",
    "    ]\n",
    "\n",
    "    # plot volcano plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(\n",
    "        result_grouped_data[\"Fold Change\"],\n",
    "        -1 * np.log10(result_grouped_data[\"P-value\"]),\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        sig_features[\"Fold Change\"],\n",
    "        -1 * np.log10(sig_features[\"P-value\"]),\n",
    "        color=\"red\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.axvline(x=1, color=\"black\", linestyle=\"--\")\n",
    "    ax.axvline(x=-1, color=\"black\", linestyle=\"--\")\n",
    "    ax.axhline(y=-np.log10(sig_threshold), color=\"black\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Fold Change\")\n",
    "    ax.set_ylabel(\"-Log10 P-value\")\n",
    "    ax.set_title(\"Volcano Plot\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def ma_plot(\n",
    "    grouped_data,\n",
    "    group_name=\"Muscle loss\",\n",
    "    group_A_name=\"control\",\n",
    "    group_B_name=\"cachexic\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an MA plot\n",
    "    \"\"\"\n",
    "    # Calculate the log-fold change and mean expression\n",
    "    cachexic_mean = (\n",
    "        grouped_data[grouped_data[group_name] == group_B_name].iloc[:, 1:].mean()\n",
    "    )\n",
    "    control_mean = (\n",
    "        grouped_data[grouped_data[group_name] == group_A_name].iloc[:, 1:].mean()\n",
    "    )\n",
    "    logFC = np.log2(cachexic_mean / control_mean)\n",
    "    data_mean = np.log2(grouped_data.iloc[:, 1:].mean(axis=0))\n",
    "    # Create the MA plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(data_mean, logFC, s=10, alpha=0.5, color=\"black\")\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Mean Expression (log2)\")\n",
    "    ax.set_ylabel(\"Log-Fold Change (log2)\")\n",
    "    ax.set_title(\"MA Plot\")\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read_data_file' from 'metabolomics_analysis_tools' (/Users/yuzhijian/opt/anaconda3/lib/python3.9/site-packages/metabolomics_analysis_tools/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPySimpleGUI\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msg\u001b[39;00m \u001b[39m# assuming that the PySimpleGUI is stored in the myapp folder\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# import the functions from the metabolomics_analysis_tools package\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmetabolomics_analysis_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     read_data_file,\n\u001b[1;32m      6\u001b[0m     normal_dist_check,\n\u001b[1;32m      7\u001b[0m     normalize_by_mean,\n\u001b[1;32m      8\u001b[0m     data_scaling_mean_centered,\n\u001b[1;32m      9\u001b[0m     data_transformation_log,\n\u001b[1;32m     10\u001b[0m     PCA_analysis,\n\u001b[1;32m     11\u001b[0m     volcano_plot,\n\u001b[1;32m     12\u001b[0m     ma_plot,\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'read_data_file' from 'metabolomics_analysis_tools' (/Users/yuzhijian/opt/anaconda3/lib/python3.9/site-packages/metabolomics_analysis_tools/__init__.py)"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg # assuming that the PySimpleGUI is stored in the myapp folder\n",
    "\n",
    "# import the functions from the metabolomics_analysis_tools package\n",
    "from metabolomics_analysis_tools import (\n",
    "    read_data_file,\n",
    "    normal_dist_check,\n",
    "    normalize_by_mean,\n",
    "    data_scaling_mean_centered,\n",
    "    data_transformation_log,\n",
    "    PCA_analysis,\n",
    "    volcano_plot,\n",
    "    ma_plot,\n",
    ")\n",
    "\n",
    "# define the layout of the GUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read successfully\n",
      "the shape of the dataframe is:  (77, 65)\n"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "def display_table(df):\n",
    "    \"\"\"\n",
    "    Display a Pandas DataFrame in a PySimpleGUI table.\n",
    "    \"\"\"\n",
    "    header_list = list(df.columns[:5]) # display only first 5 columns\n",
    "    data = df.iloc[:, :5].values.tolist() # display only first 5 columns\n",
    "    layout = [\n",
    "        [sg.Table(values=data, headings=header_list, max_col_width=25,\n",
    "                  auto_size_columns=True, justification='center',\n",
    "                  num_rows=min(25, len(data)))],\n",
    "        [sg.Button('Check Data Quality'), sg.Button('Exit')]\n",
    "    ]\n",
    "    window = sg.Window('Table', layout)\n",
    "    while True:\n",
    "        event, _ = window.read()\n",
    "        if event == 'Exit' or event == sg.WIN_CLOSED:\n",
    "            break\n",
    "        elif event == 'Check Data Quality':\n",
    "            # check data quality using the normal_dist_check function\n",
    "            if normal_dist_check(df):\n",
    "                sg.popup('The data is normally distributed.')\n",
    "            else:\n",
    "                sg.popup('The data is not normally distributed.')\n",
    "    window.close()\n",
    "\n",
    "# define the GUI layout\n",
    "layout = [\n",
    "    [sg.Text(\"Select the CSV file to read:\")],\n",
    "    [sg.Input(), sg.FileBrowse()],\n",
    "    [sg.OK(), sg.Cancel()]\n",
    "]\n",
    "\n",
    "# create the window\n",
    "window = sg.Window(\"Read CSV File\", layout)\n",
    "\n",
    "# event loop\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == \"OK\":\n",
    "        # read the CSV file using the read_data_file function\n",
    "        try:\n",
    "            df = read_data_file(file_path=values[0])\n",
    "        except FileNotFoundError:\n",
    "            sg.popup_error(\"File not found. Please select a valid CSV file.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            sg.popup_error(f\"An error occurred while reading the file: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # display the DataFrame in a table\n",
    "        display_table(df)\n",
    "    elif event == \"Cancel\" or event == sg.WIN_CLOSED:\n",
    "        break\n",
    "\n",
    "window.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c3ba5c41ad9dd74bd7117685cd73f7cb15f5937a6b2e8e86fa38521c5647524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
